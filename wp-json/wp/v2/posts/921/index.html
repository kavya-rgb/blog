{"id":921,"date":"2022-03-09T10:18:55","date_gmt":"2022-03-09T10:18:55","guid":{"rendered":"http:\/\/localhost\/wordpress\/optimizing-lambda-performance-for-your-serverless-applications\/"},"modified":"2022-03-09T10:18:55","modified_gmt":"2022-03-09T10:18:55","slug":"optimizing-lambda-performance-for-your-serverless-applications","status":"publish","type":"post","link":"http:\/\/localhost\/wordpress\/optimizing-lambda-performance-for-your-serverless-applications\/","title":{"rendered":"Optimizing Lambda Performance for Your Serverless Applications"},"content":{"rendered":"<p> <br \/>\n<\/p>\n<div data-article-id=\"1017148\" id=\"article-body\">\n<h2>\n  <a name=\"lambda-overview\" href=\"#lambda-overview\"><br \/>\n  <\/a><br \/>\n  Lambda overview<br \/>\n<\/h2>\n<p>Looking at the anatomy of a lambda function we can identify several different layers:<\/p>\n<ul>\n<li>Your function<\/li>\n<li>Language runtime<\/li>\n<li>Execution environment<\/li>\n<li>Lambda service (invisible to customers, managed by AWS)<\/li>\n<li>Compute substrate (invisible to customers, managed by AWS)<\/li>\n<\/ul>\n<p>Apart from well known Lambda runtimes (Node.js, Go etc.), Lambda also supports custom runtimes that you can bring in to Lambda service &#8211; so it&#8217;s possible to use languages like Perl, Erlang and even COBOL.<\/p>\n<p>Lambda itself is agnostic to your choice of runtime and has no preferences for one runtime over another.<\/p>\n<p>Out of 5 layers mentioned earlier, there are only three where our decisions make a difference: function itself, execution environment and Lambda service. It&#8217;s worth mentioning that AWS is consistently improving the performance of invisible layers as well.<\/p>\n<p>Quick recap of anatomy of an AWS Lambda function:<\/p>\n<ul>\n<li>Handler function &#8211; a function to be executed upon invocation<\/li>\n<li>Event object &#8211; data sent during Lambda function invocation<\/li>\n<li>Context object &#8211; methods available to interact with runtime information (request ID, log group etc.)<\/li>\n<\/ul>\n<p>There&#8217;s a difference between a full cold start (e.g. whenever a function is run for the very first time) and a partial cold start (when the execution environment is already running since it stays around for a while):<\/p>\n<p><a href=\"https:\/\/res.cloudinary.com\/practicaldev\/image\/fetch\/s--zmLGTR3T--\/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880\/https:\/\/dev-to-uploads.s3.amazonaws.com\/uploads\/articles\/32dm1uj7p8ig8625sd8k.png\" class=\"article-body-image-wrapper\"><img src=\"https:\/\/res.cloudinary.com\/practicaldev\/image\/fetch\/s--zmLGTR3T--\/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880\/https:\/\/dev-to-uploads.s3.amazonaws.com\/uploads\/articles\/32dm1uj7p8ig8625sd8k.png\" alt=\"Function lifecycle - worker host\" loading=\"lazy\" width=\"880\" height=\"481\"\/><\/a><\/p>\n<p>The first two steps (downloading the code and starting a new execution environment) are optimized by AWS, whereas the last two can be optimized by a developer.<\/p>\n<p>AWS X-Ray allows us to identify areas for performance optimization (X-Ray can both be enabled as an <code>Active tracing<\/code> checkbox in Lambda console, or via the <code>aws-xray-sdk-core<\/code>).<\/p>\n<h2>\n  <a name=\"optimizing-lambda\" href=\"#optimizing-lambda\"><br \/>\n  <\/a><br \/>\n  Optimizing Lambda<br \/>\n<\/h2>\n<h3>\n  <a name=\"three-areas-of-performance\" href=\"#three-areas-of-performance\"><br \/>\n  <\/a><br \/>\n  Three areas of performance<br \/>\n<\/h3>\n<h3>\n  <a name=\"latency-cold-starts\" href=\"#latency-cold-starts\"><br \/>\n  <\/a><br \/>\n  Latency &#8211; cold starts<br \/>\n<\/h3>\n<p>A Lambda function call always starts with a request to Lambda&#8217;s API. The next steps depend on whether there is a warm execution environment available.<\/p>\n<p>If there is one, that means that the Lambda function was executed rather recently and the handler can be invoked on that execution environment.<\/p>\n<p>This becomes a bit more complicated if a warm execution environment is not available:<\/p>\n<p><a href=\"https:\/\/res.cloudinary.com\/practicaldev\/image\/fetch\/s--KrJheCN3--\/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880\/https:\/\/dev-to-uploads.s3.amazonaws.com\/uploads\/articles\/gvactw51zglf83fgm18i.png\" class=\"article-body-image-wrapper\"><img src=\"https:\/\/res.cloudinary.com\/practicaldev\/image\/fetch\/s--KrJheCN3--\/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880\/https:\/\/dev-to-uploads.s3.amazonaws.com\/uploads\/articles\/gvactw51zglf83fgm18i.png\" alt=\"Function lifecycle - a full cold start\" loading=\"lazy\" width=\"880\" height=\"489\"\/><\/a><\/p>\n<p>The INIT code is referring to code that is defined outside of the <code>handler<\/code> function.<\/p>\n<p>Cold starts affect less than 1% of production workloads (they are much more common in development environments, since there is way less traffic). A duration of a cold start varies from less than 100ms to more than 1s. It&#8217;s not possible to &#8216;target&#8217; warm environments.<\/p>\n<p>The only actual guarantee of getting a warm environment is using provisioned concurrency, pinging functions to keep the warm will not work when function is scaling out, or the execution environment is being load balanced to a different AZ.<\/p>\n<p>Making function packages as small as possible helps (since there&#8217;s simply less code to download).<\/p>\n<p>Generally speaking, functions executed more often than others will be hitting warm environments more frequently.<\/p>\n<p>Static initialization is influenced by:<\/p>\n<ul>\n<li>size of function package<\/li>\n<li>amount of code<\/li>\n<li>amount of initialization work (that is &#8211; code that runs before the <code>handler<\/code> function)<\/li>\n<\/ul>\n<p><strong>The developer is responsible for this part of a cold start<\/strong><\/p>\n<p>What can help? Trimming SDKs, reusing DB connections, not loading things we don&#8217;t need and lazy loading.<\/p>\n<p>If you absolutely need to have warm execution environments, use <strong>Provisioned Concurrency on AWS Lambda<\/strong> (announced at Re:Invent 2019):<\/p>\n<p><a href=\"https:\/\/res.cloudinary.com\/practicaldev\/image\/fetch\/s--23PB5-qN--\/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880\/https:\/\/dev-to-uploads.s3.amazonaws.com\/uploads\/articles\/f4djoq0mgffpnakcqf6u.png\" class=\"article-body-image-wrapper\"><img src=\"https:\/\/res.cloudinary.com\/practicaldev\/image\/fetch\/s--23PB5-qN--\/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880\/https:\/\/dev-to-uploads.s3.amazonaws.com\/uploads\/articles\/f4djoq0mgffpnakcqf6u.png\" alt=\"Provisioned Concurrency on AWS Lambda\" loading=\"lazy\" width=\"880\" height=\"496\"\/><\/a><\/p>\n<p>Provisioned concurrency has a rampup of 500 execution environments per minute (so if you need 5k execution environments it&#8217;ll take 10 minutes to spin them up).<\/p>\n<p>It&#8217;s possible to have different provisioned concurrency for different versions of the Lambda function (and remember that it&#8217;s not possible to use provisioned concurrency for <code>$LATEST<\/code> version).<\/p>\n<h3>\n  <a name=\"memory-and-profiling\" href=\"#memory-and-profiling\"><br \/>\n  <\/a><br \/>\n  Memory and profiling<br \/>\n<\/h3>\n<p>Lambda exposes only memory control (as it&#8217;s not possible to modify anything else that influences the &#8216;power&#8217; of a Lambda function). The default is 128MB, which is well suited for small Lambda functions, but its definitely possible to save money\/time by assigning a larger memory for your function:<\/p>\n<p><a href=\"https:\/\/res.cloudinary.com\/practicaldev\/image\/fetch\/s--M-v4yKQy--\/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880\/https:\/\/dev-to-uploads.s3.amazonaws.com\/uploads\/articles\/hfoepxq891j0nyqr2j4u.png\" class=\"article-body-image-wrapper\"><img src=\"https:\/\/res.cloudinary.com\/practicaldev\/image\/fetch\/s--M-v4yKQy--\/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880\/https:\/\/dev-to-uploads.s3.amazonaws.com\/uploads\/articles\/hfoepxq891j0nyqr2j4u.png\" alt=\"CPU-bound example\" loading=\"lazy\" width=\"880\" height=\"495\"\/><\/a><\/p>\n<p>AWS Lambda Power Tuning allows the developer to optimize their lambda function from cost\/performance perspective without running manual experiments.<\/p>\n<h3>\n  <a name=\"architecture-and-best-practices\" href=\"#architecture-and-best-practices\"><br \/>\n  <\/a><br \/>\n  Architecture and best practices<br \/>\n<\/h3>\n<ul>\n<li>Avoid &#8220;monolithic&#8221; functions (optimize package size, micro\/nano services)<\/li>\n<li>Minifiy\/uglify production code<\/li>\n<li>Optimize dependencies and imports<\/li>\n<li>Lazy initialization of shared libraries\/objects<\/li>\n<\/ul>\n<p>It&#8217;s better to have a lambda function per purpose rather than one giant Lambda that is essentially <code>yourBusinessLogic.js<\/code>.<\/p>\n<p>In order to solve issues with connections to a relational database, consider using an RDS Proxy:<\/p>\n<p><a href=\"https:\/\/res.cloudinary.com\/practicaldev\/image\/fetch\/s--N_eVJd49--\/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880\/https:\/\/dev-to-uploads.s3.amazonaws.com\/uploads\/articles\/gwaey6oryvj6mjb44lfu.png\" class=\"article-body-image-wrapper\"><img src=\"https:\/\/res.cloudinary.com\/practicaldev\/image\/fetch\/s--N_eVJd49--\/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880\/https:\/\/dev-to-uploads.s3.amazonaws.com\/uploads\/articles\/gwaey6oryvj6mjb44lfu.png\" alt=\"Amazon RDS Proxy\" loading=\"lazy\" width=\"880\" height=\"493\"\/><\/a><\/p>\n<p>RDS Proxy (launched at Re:Invent 2019) will intelligently manage your connections to a relational database in RDS. As such, it helps match the way that scalable compute works with traditional database architectures.<\/p>\n<p>In order to reuse existing connections (and improve the performance of functions using <code>http(s)<\/code> request) use <code>keep-alive<\/code> property in order to reuse TCP connections on warm execution environments. More details: <a href=\"https:\/\/bit.ly\/reuse-connection\">https:\/\/bit.ly\/reuse-connection<\/a><\/p>\n<h3>\n  <a name=\"sync-vs-async-lambda-architectures\" href=\"#sync-vs-async-lambda-architectures\"><br \/>\n  <\/a><br \/>\n  Sync vs. async lambda architectures<br \/>\n<\/h3>\n<p><a href=\"https:\/\/res.cloudinary.com\/practicaldev\/image\/fetch\/s--fUs02gmK--\/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880\/https:\/\/dev-to-uploads.s3.amazonaws.com\/uploads\/articles\/kch80cda3gm48loebryu.png\" class=\"article-body-image-wrapper\"><img src=\"https:\/\/res.cloudinary.com\/practicaldev\/image\/fetch\/s--fUs02gmK--\/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880\/https:\/\/dev-to-uploads.s3.amazonaws.com\/uploads\/articles\/kch80cda3gm48loebryu.png\" alt=\"Comparing sync and async\" loading=\"lazy\" width=\"880\" height=\"494\"\/><\/a><\/p>\n<p>Source for all screenshots: <a href=\"https:\/\/www.youtube.com\/watch?v=FTCaOQJvG6Y\">Optimizing Lambda Performance for Your Serverless Applications<\/a><\/p>\n<h2>\n  <a name=\"stay-on-top-of-your-logs-%EF%B8%8F\" href=\"#stay-on-top-of-your-logs-%EF%B8%8F\"><br \/>\n  <\/a><br \/>\n  Stay on top of your logs. \u26a1\ufe0f<br \/>\n<\/h2>\n<p><a href=\"https:\/\/res.cloudinary.com\/practicaldev\/image\/fetch\/s--RXm-FJPk--\/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880\/https:\/\/cloudash.dev\/_next\/image%3Furl%3D%252F_next%252Fstatic%252Fmedia%252Fcloudash_function.6e0454ea.png%26w%3D3840%26q%3D75\" class=\"article-body-image-wrapper\"><img src=\"https:\/\/res.cloudinary.com\/practicaldev\/image\/fetch\/s--RXm-FJPk--\/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880\/https:\/\/cloudash.dev\/_next\/image%3Furl%3D%252F_next%252Fstatic%252Fmedia%252Fcloudash_function.6e0454ea.png%26w%3D3840%26q%3D75\" alt=\"Cloudash screenshot\" loading=\"lazy\" width=\"880\" height=\"631\"\/><\/a><\/p>\n<p>Introducing <a href=\"https:\/\/cloudash.dev\">Cloudash<\/a>, a desktop app for monitoring your serverless services performance, invocations, errors and more.<\/p>\n<p>Did a production incident happen last week? Or 20 seconds ago? With Cloudash you can search, filter and browse your serverless logs and metrics effortlessly.<\/p>\n<p>Search for whatever you want, whenever you want. Cloudash comes with built-in filtering capabilities enabling to get to the bottom of your problems faster than ever before.<\/p>\n<p>Get started <a href=\"https:\/\/cloudash.dev\">here<\/a>.<\/p>\n<\/p><\/div>\n<p><br \/>\n<br \/><a href=\"https:\/\/dev.to\/tlakomy\/optimizing-lambda-performance-for-your-serverless-applications-44lo\">Source link <\/a><\/p>\n","protected":false},"excerpt":{"rendered":"<p>Lambda overview Looking at the anatomy of a lambda function we can identify several different layers: Your function Language runtime Execution environment Lambda service (invisible to customers, managed by AWS) Compute substrate (invisible to customers, managed by AWS) Apart from well known Lambda runtimes (Node.js, Go etc.), Lambda also supports custom runtimes that you can &hellip;<\/p>\n<p class=\"read-more\"> <a class=\"\" href=\"http:\/\/localhost\/wordpress\/optimizing-lambda-performance-for-your-serverless-applications\/\"> <span class=\"screen-reader-text\">Optimizing Lambda Performance for Your Serverless Applications<\/span> Read More &raquo;<\/a><\/p>\n","protected":false},"author":1,"featured_media":922,"comment_status":"open","ping_status":"open","sticky":false,"template":"","format":"standard","meta":{"site-sidebar-layout":"default","site-content-layout":"default","ast-main-header-display":"","ast-hfb-above-header-display":"","ast-hfb-below-header-display":"","ast-hfb-mobile-header-display":"","site-post-title":"","ast-breadcrumbs-content":"","ast-featured-img":"","footer-sml-layout":"","theme-transparent-header-meta":"","adv-header-id-meta":"","stick-header-meta":"","header-above-stick-meta":"","header-main-stick-meta":"","header-below-stick-meta":""},"categories":[1],"tags":[],"_links":{"self":[{"href":"http:\/\/localhost\/wordpress\/wp-json\/wp\/v2\/posts\/921"}],"collection":[{"href":"http:\/\/localhost\/wordpress\/wp-json\/wp\/v2\/posts"}],"about":[{"href":"http:\/\/localhost\/wordpress\/wp-json\/wp\/v2\/types\/post"}],"author":[{"embeddable":true,"href":"http:\/\/localhost\/wordpress\/wp-json\/wp\/v2\/users\/1"}],"replies":[{"embeddable":true,"href":"http:\/\/localhost\/wordpress\/wp-json\/wp\/v2\/comments?post=921"}],"version-history":[{"count":0,"href":"http:\/\/localhost\/wordpress\/wp-json\/wp\/v2\/posts\/921\/revisions"}],"wp:featuredmedia":[{"embeddable":true,"href":"http:\/\/localhost\/wordpress\/wp-json\/wp\/v2\/media\/922"}],"wp:attachment":[{"href":"http:\/\/localhost\/wordpress\/wp-json\/wp\/v2\/media?parent=921"}],"wp:term":[{"taxonomy":"category","embeddable":true,"href":"http:\/\/localhost\/wordpress\/wp-json\/wp\/v2\/categories?post=921"},{"taxonomy":"post_tag","embeddable":true,"href":"http:\/\/localhost\/wordpress\/wp-json\/wp\/v2\/tags?post=921"}],"curies":[{"name":"wp","href":"https:\/\/api.w.org\/{rel}","templated":true}]}}